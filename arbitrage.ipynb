{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('arbitrage_features.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['spread1'] = df['near_price'] - df['far_price']\n",
    "raw_features = df[['spread1', 'turnover_ratio', 'vol_curve', 'tick_imbalance']].dropna()\n",
    "df['future_spread_change'] = df['spread'].shift(-5) - df['spread']\n",
    "df['signal'] = np.where(df['future_spread_change'] > 0.002, 1,\n",
    "                 np.where(df['future_spread_change'] < -0.002, -1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler(quantile_range=(25, 75))\n",
    "scaled_features = scaler.fit_transform(raw_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, window_size=60, step_size=10):\n",
    "    sequences = []\n",
    "    for i in range(0, len(data) - window_size, step_size):\n",
    "        sequences.append(data[i:i+window_size])\n",
    "    return np.array(sequences)\n",
    "\n",
    "# 60 timestamps（1min ticks）\n",
    "X = create_sequences(scaled_features, window_size=60)\n",
    "y = df['signal'][60:-5:10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_tensor = torch.FloatTensor(X)\n",
    "y_tensor = torch.LongTensor(y)\n",
    "\n",
    "split_idx = int(len(X_tensor) * 0.8)\n",
    "train_X, val_X = X_tensor[:split_idx], X_tensor[split_idx:]\n",
    "train_y, val_y = y_tensor[:split_idx], y_tensor[split_idx:]\n",
    "\n",
    "train_dataset = TensorDataset(train_X, train_y)\n",
    "val_dataset = TensorDataset(val_X, val_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArbitrageTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=4, d_model=64, nhead=8, num_layers=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward=128, dropout=dropout)\n",
    "        self.transformer = TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.decoder = nn.Linear(d_model, 3)  # 3 outputs：-1, 0, 1\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer(src, src_mask)\n",
    "        last_output = output[-1]\n",
    "        return self.decoder(last_output)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # create (max_len, d_model) position encoding matrix\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))  # (d_model/2,)\n",
    "\n",
    "        pe = torch.zeros(max_len, 1, d_model)  # (max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)  # even position\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)  # odd position\n",
    "\n",
    "        self.register_buffer('pe', pe)  \n",
    "\n",
    "    def forward(self, x):\n",
   
    "        x = x + self.pe[:x.size(0)] \n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(seq, volatility_threshold=0.1):\n",
   
    "    seq_len, _ = seq.shape\n",
    "    mask = torch.ones(seq_len, seq_len, dtype=torch.bool)\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        if seq[i].mean().item() > volatility_threshold:\n",
    "            mask[i, max(0, i-10):i+1] = False\n",
    "        else:\n",
    "            mask[i, :i+1] = False \n",
    "    \n",
    "    return mask.to(seq.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = ArbitrageTransformer().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3,\n",
    "                                                steps_per_epoch=len(train_loader), epochs=20)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        mask = create_mask(batch_X[:, :, 1].transpose(0, 1))\n",
    "        batch_X = batch_X.transpose(0, 1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_X, src_mask=mask)\n",
    "        loss = loss_fn(output, batch_y + 1)  # shift label from [-1,0,1] to [0,1,2]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            mask = create_mask(batch_X[:, :, 1].transpose(0, 1))\n",
    "            output = model(batch_X.transpose(0, 1), src_mask=mask)\n",
    "            loss = loss_fn(output, batch_y + 1)\n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == (batch_y + 1)).sum().item()\n",
    "    return total_loss / len(loader), correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=1.0748, Val Loss=1.0814\n",
      "Epoch 2: Train Loss=1.0715, Val Loss=1.0817\n",
      "Epoch 3: Train Loss=1.0706, Val Loss=1.0816\n",
      "Epoch 4: Train Loss=1.0697, Val Loss=1.0815\n",
      "Epoch 5: Train Loss=1.0693, Val Loss=1.0816\n",
      "Epoch 6: Train Loss=1.0692, Val Loss=1.0816\n",
      "Epoch 7: Train Loss=1.0692, Val Loss=1.0817\n",
      "Epoch 8: Train Loss=1.0692, Val Loss=1.0817\n",
      "Epoch 9: Train Loss=1.0692, Val Loss=1.0817\n",
      "Epoch 10: Train Loss=1.0691, Val Loss=1.0817\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "for epoch in range(10):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, loss_fn, device)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model_class3.pth\")\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return: 8.42%\n",
      "Max Drawdown: 9.14%\n",
      "Sharpe: 0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/62/pm6rwllx53d08gq5xbp3_njr0000gn/T/ipykernel_14024/398240959.py:29: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  sharpe_ratio = annual_return / volatility\n"
     ]
    }
   ],
   "source": [
    "model = ArbitrageTransformer().to(device)\n",
    "model.load_state_dict(torch.load(\"best_model_class3.pth\"))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(X_tensor), 256):\n",
    "        batch = X_tensor[i:i+256].transpose(0, 1)  # shape: (seq_len, batch, features)\n",
    "        output = model(batch)  # shape: (batch, 3)\n",
    "        pred = output.argmax(dim=1).cpu().numpy() - 1  # map back to [-1, 0, 1]\n",
    "        all_preds.extend(pred)\n",
    "\n",
    "signals = np.array(all_preds)\n",
    "\n",
    "backtest_df = df.iloc[60:-5:10].copy() \n",
    "backtest_df = backtest_df.iloc[:len(signals)].copy()\n",
    "backtest_df['signal'] = signals\n",
    "spread_change = backtest_df['spread1'].pct_change(5).shift(-5)  \n",
    "backtest_df['strategy_return'] = backtest_df['signal'] * spread_change\n",
    "backtest_df['strategy_return'] = backtest_df['strategy_return'].fillna(0)\n",
    "\n",
    "cumulative_return = (1 + backtest_df['strategy_return']).cumprod()\n",
    "max_drawdown = (cumulative_return / cumulative_return.cummax() - 1).min()\n",
    "annual_return = cumulative_return.iloc[-1]**(252*24*6 / len(cumulative_return)) - 1\n",
    "volatility = backtest_df['strategy_return'].std() * np.sqrt(252*24*6)\n",
    "sharpe_ratio = annual_return / volatility\n",
    "\n",
    "print(f\"Retuen: {(cumulative_return.iloc[-1]-1):.2%}\")\n",
    "print(f\"Max Drawdown: {max_drawdown:.2%}\")\n",
    "print(f\"Sharpe: {sharpe_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
